{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#reset tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Import Data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "#Training & Test Data\n",
    "trainX = mnist.train.images\n",
    "trainY = mnist.train.labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Work\\Anaconda3\\envs\\tensorflow-cpu\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import tensorflow as tf\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX,testX,trainY,testY=train_test_split(trainX, trainY, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44000, 784)\n",
      "(44000, 10)\n",
      "(11000, 784)\n",
      "(11000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "print(testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trainX=trainX/255\n",
    "#testX=testX/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Classifier Parameters\n",
    "logs_path='/tmp/session45/v13'\n",
    "save_path=logs_path + '/'\n",
    "learning_rate = 0.003 #0.003, 0.0001, 0.0002\n",
    "n_features = 784\n",
    "n_classes = 10\n",
    "batch_size = 100\n",
    "training_epochs = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Defining Hidden Layers\n",
    "K = 400\n",
    "L = 300\n",
    "M = 200\n",
    "N = 100\n",
    "O=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('input'):\n",
    "    # None -> batch size can be any size, with n_features\n",
    "    x = tf.placeholder(tf.float32, shape=[None, n_features], name=\"x-input\") \n",
    "    # target n_classes output classes\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, n_classes], name=\"y-input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Defining Swish Activation\n",
    "def swish(x):\n",
    "    return x*tf.nn.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    beta=tf.Variable(initial_value=1.0,trainable=True,name='swish')\n",
    "    return x*tf.nn.sigmoid(beta*x) #trainable parameter beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Layer1\"):\n",
    "    W1 = tf.Variable(tf.truncated_normal([n_features, K] ,stddev=0.1))\n",
    "    b1 = tf.Variable(tf.ones([K]))\n",
    "    #Y1 = tf.nn.sigmoid(tf.matmul(x, W1) + b1)\n",
    "    #Y1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "    Y1 = swish(tf.add(tf.matmul(x, W1),b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Layer2\"):\n",
    "    W2 = tf.Variable(tf.truncated_normal([K, L] ,stddev=0.1))\n",
    "    b2 = tf.Variable(tf.ones([L]))\n",
    "    #Y2 = tf.nn.sigmoid(tf.matmul(Y1, W2) + b2)\n",
    "    #Y2 = tf.nn.relu(tf.matmul(Y1, W2) + b2)\n",
    "    Y2 = swish(tf.add(tf.matmul(Y1, W2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Layer3\"):\n",
    "    W3 = tf.Variable(tf.truncated_normal([L, M] ,stddev=0.1))\n",
    "    b3 = tf.Variable(tf.ones([M]))\n",
    "    #Y3 = tf.nn.sigmoid(tf.matmul(Y2, W3) + b3)\n",
    "    #Y3 = tf.nn.relu(tf.matmul(Y2, W3) + b3)\n",
    "    Y3 = swish(tf.add(tf.matmul(Y2, W3), b3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Layer4\"):\n",
    "    W4 = tf.Variable(tf.truncated_normal([M, N] ,stddev=0.1))\n",
    "    b4 = tf.Variable(tf.ones([N]))\n",
    "    #Y4 = tf.nn.sigmoid(tf.matmul(Y3, W4) + b4)\n",
    "    #Y4 = tf.nn.relu(tf.matmul(Y3, W4) + b4)\n",
    "    Y4 = swish(tf.add(tf.matmul(Y3, W4), b4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Layer5\"):\n",
    "    W5=tf.Variable(tf.truncated_normal([N,O], stddev=0.1))\n",
    "    b5=tf.Variable(tf.ones([O]))\n",
    "    Y5=swish(tf.add(tf.matmul(Y4,W5),b5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implement model\n",
    "with tf.name_scope(\"Output\"):\n",
    "    # y is our prediction\n",
    "    W = tf.Variable(tf.truncated_normal([O, n_classes] ,stddev=0.1))\n",
    "    b = tf.Variable(tf.zeros([n_classes]))   \n",
    "    #y = tf.nn.softmax(tf.matmul(Y5, W) + b)\n",
    "    Ylogits = tf.matmul(Y5, W) + b\n",
    "    y = tf.nn.softmax(Ylogits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify cost function\n",
    "with tf.name_scope('Loss'):\n",
    "    # this is our cost\n",
    "    #cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=y_)\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify optimizer\n",
    "with tf.name_scope('train'):\n",
    "    # optimizer is an \"operation\" which we can execute in a session\n",
    "    learn_rate = tf.Variable(learning_rate)\n",
    "    train_op = tf.train.AdadeltaOptimizer(learn_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Accuracy'):\n",
    "    # Prediction\n",
    "    prediction = tf.argmax(y,1,name=\"Predict\")\n",
    "    #Accuracy\n",
    "    correct_prediction = tf.equal(prediction, tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a summary for our cost and accuracy\n",
    "training_loss = tf.summary.scalar(\"training_loss\", cross_entropy)\n",
    "training_accuracy = tf.summary.scalar(\"training_accuracy\", accuracy)\n",
    "test_loss = tf.summary.scalar(\"test_loss\", cross_entropy)\n",
    "test_accuracy = tf.summary.scalar(\"test_accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a Saver to save the graph\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss at step:  0  is  250.778\n",
      "Epoch:  0\n",
      "Test loss at step:  1  is  214.695\n",
      "Test loss at step:  2  is  193.717\n",
      "Test loss at step:  3  is  176.513\n",
      "Test loss at step:  4  is  161.131\n",
      "Test loss at step:  5  is  147.319\n",
      "Epoch:  5\n",
      "Test loss at step:  6  is  134.91\n",
      "Test loss at step:  7  is  123.767\n",
      "Test loss at step:  8  is  113.819\n",
      "Test loss at step:  9  is  105.02\n",
      "Test loss at step:  10  is  97.3112\n",
      "Epoch:  10\n",
      "Test loss at step:  11  is  90.5978\n",
      "Test loss at step:  12  is  84.7638\n",
      "Test loss at step:  13  is  79.6909\n",
      "Test loss at step:  14  is  75.2691\n",
      "Test loss at step:  15  is  71.401\n",
      "Epoch:  15\n",
      "Test loss at step:  16  is  68.0024\n",
      "Test loss at step:  17  is  65.0017\n",
      "Test loss at step:  18  is  62.3385\n",
      "Test loss at step:  19  is  59.9627\n",
      "Test loss at step:  20  is  57.8322\n",
      "Epoch:  20\n",
      "Test loss at step:  21  is  55.9122\n",
      "Test loss at step:  22  is  54.1737\n",
      "Test loss at step:  23  is  52.5927\n",
      "Test loss at step:  24  is  51.149\n",
      "Test loss at step:  25  is  49.8258\n",
      "Epoch:  25\n",
      "Test loss at step:  26  is  48.6089\n",
      "Test loss at step:  27  is  47.4862\n",
      "Test loss at step:  28  is  46.4473\n",
      "Test loss at step:  29  is  45.4832\n",
      "Test loss at step:  30  is  44.5862\n",
      "Epoch:  30\n",
      "Test loss at step:  31  is  43.7493\n",
      "Test loss at step:  32  is  42.9666\n",
      "Test loss at step:  33  is  42.2331\n",
      "Test loss at step:  34  is  41.5441\n",
      "Test loss at step:  35  is  40.8955\n",
      "Epoch:  35\n",
      "Test loss at step:  36  is  40.2838\n",
      "Test loss at step:  37  is  39.7059\n",
      "Test loss at step:  38  is  39.1588\n",
      "Test loss at step:  39  is  38.6402\n",
      "Test loss at step:  40  is  38.1476\n",
      "Epoch:  40\n",
      "Test loss at step:  41  is  37.6792\n",
      "Test loss at step:  42  is  37.233\n",
      "Test loss at step:  43  is  36.8074\n",
      "Test loss at step:  44  is  36.4009\n",
      "Test loss at step:  45  is  36.0121\n",
      "Epoch:  45\n",
      "Test loss at step:  46  is  35.6399\n",
      "Test loss at step:  47  is  35.283\n",
      "Test loss at step:  48  is  34.9405\n",
      "Test loss at step:  49  is  34.6113\n",
      "Test loss at step:  50  is  34.2948\n",
      "Epoch:  50\n",
      "Test loss at step:  51  is  33.9899\n",
      "Test loss at step:  52  is  33.6961\n",
      "Test loss at step:  53  is  33.4126\n",
      "Test loss at step:  54  is  33.1389\n",
      "Test loss at step:  55  is  32.8743\n",
      "Epoch:  55\n",
      "Test loss at step:  56  is  32.6184\n",
      "Test loss at step:  57  is  32.3706\n",
      "Test loss at step:  58  is  32.1306\n",
      "Test loss at step:  59  is  31.8977\n",
      "Test loss at step:  60  is  31.6718\n",
      "Epoch:  60\n",
      "Test loss at step:  61  is  31.4524\n",
      "Test loss at step:  62  is  31.2391\n",
      "Test loss at step:  63  is  31.0317\n",
      "Test loss at step:  64  is  30.8299\n",
      "Test loss at step:  65  is  30.6333\n",
      "Epoch:  65\n",
      "Test loss at step:  66  is  30.4418\n",
      "Test loss at step:  67  is  30.2551\n",
      "Test loss at step:  68  is  30.0729\n",
      "Test loss at step:  69  is  29.895\n",
      "Test loss at step:  70  is  29.7213\n",
      "Epoch:  70\n",
      "Test loss at step:  71  is  29.5516\n",
      "Test loss at step:  72  is  29.3856\n",
      "Test loss at step:  73  is  29.2232\n",
      "Test loss at step:  74  is  29.0643\n",
      "Test loss at step:  75  is  28.9087\n",
      "Epoch:  75\n",
      "Test loss at step:  76  is  28.7563\n",
      "Test loss at step:  77  is  28.6069\n",
      "Test loss at step:  78  is  28.4605\n",
      "Test loss at step:  79  is  28.3168\n",
      "Test loss at step:  80  is  28.1758\n",
      "Epoch:  80\n",
      "Test loss at step:  81  is  28.0375\n",
      "Test loss at step:  82  is  27.9016\n",
      "Test loss at step:  83  is  27.7682\n",
      "Test loss at step:  84  is  27.6369\n",
      "Test loss at step:  85  is  27.508\n",
      "Epoch:  85\n",
      "Test loss at step:  86  is  27.3812\n",
      "Test loss at step:  87  is  27.2565\n",
      "Test loss at step:  88  is  27.1338\n",
      "Test loss at step:  89  is  27.013\n",
      "Test loss at step:  90  is  26.8941\n",
      "Epoch:  90\n",
      "Test loss at step:  91  is  26.777\n",
      "Test loss at step:  92  is  26.6616\n",
      "Test loss at step:  93  is  26.548\n",
      "Test loss at step:  94  is  26.4359\n",
      "Test loss at step:  95  is  26.3255\n",
      "Epoch:  95\n",
      "Test loss at step:  96  is  26.2167\n",
      "Test loss at step:  97  is  26.1092\n",
      "Test loss at step:  98  is  26.0033\n",
      "Test loss at step:  99  is  25.8988\n",
      "Test loss at step:  100  is  25.7957\n",
      "Epoch:  100\n",
      "Test loss at step:  101  is  25.6939\n",
      "Test loss at step:  102  is  25.5934\n",
      "Test loss at step:  103  is  25.4941\n",
      "Test loss at step:  104  is  25.3961\n",
      "Test loss at step:  105  is  25.2993\n",
      "Epoch:  105\n",
      "Test loss at step:  106  is  25.2036\n",
      "Test loss at step:  107  is  25.1091\n",
      "Test loss at step:  108  is  25.0157\n",
      "Test loss at step:  109  is  24.9234\n",
      "Test loss at step:  110  is  24.8321\n",
      "Epoch:  110\n",
      "Test loss at step:  111  is  24.7419\n",
      "Test loss at step:  112  is  24.6526\n",
      "Test loss at step:  113  is  24.5644\n",
      "Test loss at step:  114  is  24.4771\n",
      "Test loss at step:  115  is  24.3907\n",
      "Epoch:  115\n",
      "Test loss at step:  116  is  24.3053\n",
      "Test loss at step:  117  is  24.2208\n",
      "Test loss at step:  118  is  24.1371\n",
      "Test loss at step:  119  is  24.0543\n",
      "Test loss at step:  120  is  23.9724\n",
      "Epoch:  120\n",
      "Test loss at step:  121  is  23.8913\n",
      "Test loss at step:  122  is  23.811\n",
      "Test loss at step:  123  is  23.7315\n",
      "Test loss at step:  124  is  23.6527\n",
      "Test loss at step:  125  is  23.5748\n",
      "Epoch:  125\n",
      "Test loss at step:  126  is  23.4976\n",
      "Test loss at step:  127  is  23.4211\n",
      "Test loss at step:  128  is  23.3453\n",
      "Test loss at step:  129  is  23.2703\n",
      "Test loss at step:  130  is  23.1959\n",
      "Epoch:  130\n",
      "Test loss at step:  131  is  23.1223\n",
      "Test loss at step:  132  is  23.0493\n",
      "Test loss at step:  133  is  22.9769\n",
      "Test loss at step:  134  is  22.9053\n",
      "Test loss at step:  135  is  22.8342\n",
      "Epoch:  135\n",
      "Test loss at step:  136  is  22.7638\n",
      "Test loss at step:  137  is  22.694\n",
      "Test loss at step:  138  is  22.6248\n",
      "Test loss at step:  139  is  22.5563\n",
      "Test loss at step:  140  is  22.4883\n",
      "Epoch:  140\n",
      "Test loss at step:  141  is  22.4208\n",
      "Test loss at step:  142  is  22.354\n",
      "Test loss at step:  143  is  22.2877\n",
      "Test loss at step:  144  is  22.222\n",
      "Test loss at step:  145  is  22.1568\n",
      "Epoch:  145\n",
      "Test loss at step:  146  is  22.0922\n",
      "Test loss at step:  147  is  22.0281\n",
      "Test loss at step:  148  is  21.9645\n",
      "Test loss at step:  149  is  21.9015\n",
      "Test loss at step:  150  is  21.8389\n",
      "Epoch:  150\n",
      "Test loss at step:  151  is  21.7769\n",
      "Test loss at step:  152  is  21.7153\n",
      "Test loss at step:  153  is  21.6543\n",
      "Test loss at step:  154  is  21.5937\n",
      "Test loss at step:  155  is  21.5336\n",
      "Epoch:  155\n",
      "Test loss at step:  156  is  21.474\n",
      "Test loss at step:  157  is  21.4148\n",
      "Test loss at step:  158  is  21.3561\n",
      "Test loss at step:  159  is  21.2978\n",
      "Accuracy:  0.937273\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Start Graph execution\n",
    "with tf.Session() as sess:\n",
    "    # variables need to be initialized before we can use them\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # create log writer object\n",
    "    writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "    # perform training cycles\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # number of batches in one epoch\n",
    "        batch_count = int(trainX.shape[0]/batch_size)\n",
    "        \n",
    "        for i in range(batch_count):\n",
    "            batch_x  = trainX[i*batch_size:i*batch_size+batch_size]\n",
    "            batch_y  = trainY[i*batch_size:i*batch_size+batch_size]\n",
    "\n",
    "            # perform the operations we defined earlier on batch\n",
    "            _,acc,loss = sess.run([train_op, training_accuracy,training_loss], feed_dict={x: batch_x, y_: batch_y})\n",
    "            \n",
    "            #log training accuracy and loss\n",
    "            writer.add_summary(acc, epoch * batch_count + i)\n",
    "            writer.add_summary(loss, epoch * batch_count + i)    \n",
    "                        \n",
    "        #Test loss and accuracy\n",
    "        acc,loss,a_loss = sess.run([test_accuracy,test_loss,cross_entropy],\n",
    "                                   feed_dict={x: testX, y_: testY})\n",
    "        writer.add_summary(acc, epoch * batch_count + i)\n",
    "        writer.add_summary(loss, epoch * batch_count + i)\n",
    "        print ('Test loss at step: ', epoch, ' is ', a_loss) \n",
    "        if epoch % 5 == 0: \n",
    "            print (\"Epoch: \", epoch)\n",
    "                \n",
    "    print (\"Accuracy: \", accuracy.eval(feed_dict={x: testX, y_: testY}))\n",
    "    \n",
    "    #Save the model\n",
    "    saver.save(sess, save_path + \"model.ckpt\")\n",
    "    print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "665.3527445754144\n"
     ]
    }
   ],
   "source": [
    "stop = timeit.default_timer()\n",
    "\n",
    "print (stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
