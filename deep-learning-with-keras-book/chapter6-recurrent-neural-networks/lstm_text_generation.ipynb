{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Recurrent Neural Network\n",
    "\n",
    "This notebook is a replica of Chapter 6: Recurrent Neural Network from the Deep Learning with Keras book by Antonio Gulli, Sujit Paul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs have been used extensively by the **natural language processing (NLP)** community for various applications. One such application is building language models. A language model allows us to predict the probability of a word in a text given the previous words. Language models are important for various higher level tasks such as machine translation, spelling correction, and so on.\n",
    "\n",
    "A side effect of the ability to predict the next word given previous words is a generative model that allows us to generate text by sampling from the output probabilities. In language modeling, our input is typically a sequence of words and the output is a sequence of predicted words. The training data used is existing unlabeled text, where we set the label $y_t$ at time $t$ to be the input $x_{t+1}$ at time $t+1$.\n",
    "\n",
    "In this example of using Keras for building LSTMs, we will train a character based language model on the text of *Alice in Wonderland* to predict the next character given 10 previous characters. We have chosen to build a character-based model here because it has a smaller vocabulary and trains quicker. The idea is the same as using a word-based language model, except we use characters instead of words. We will then use the trained model to generate some text in the same style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import collections\n",
    "\n",
    "import re, nltk\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "\n",
    "# this allows plots to appear directly in the notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read our input text from the text of Alice in Wonderland on the Project Gutenberg website (http://www.gutenberg.org/files/11/11-0.txt). The file contains line breaks and non- ASCII characters, so we do some preliminary cleanup and write out the contents into a variable called `text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"data/alice_in_wonderland.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the input as a stream of characters\n",
    "lines = []\n",
    "with open(file=INPUT_FILE) as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip().lower()\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        lines.append(line)\n",
    "text = \" \".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are building a character-level LSTM, our vocabulary is the set of characters that occur in the text. There are 60 of them in our case. Since we will be dealing with the indexes to these characters rather than the characters themselves, the following code snippet creates the necessary lookup tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here chars is the number of features in our character \"vocabulary\"\n",
    "chars = sorted(list(set(text)))\n",
    "nb_chars = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '‘', '’', '“', '”', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocab: 60\n"
     ]
    }
   ],
   "source": [
    "print('Total vocab:', nb_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating lookup tables i.e. mapping of unique chars to integers\n",
    "char2index = dict((c, i) for i, c in enumerate(chars))\n",
    "index2char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create the input and label texts. We do this by stepping through the text by a number of characters given by the `STEP` variable (`1` in our case) and then extracting a span of text whose size is determined by the `SEQLEN` variable (`10` in our case). The next character after the span is our label character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inputs and labels from the text. We do this by stepping\n",
    "# through the text ${step} character at a time, and extracting a \n",
    "# sequence of size ${seqlen} and the next output char. For example,\n",
    "# assuming an input text \"The sky was falling\", we would get the \n",
    "# following sequence of input_chars and label_chars (first 5 only)\n",
    "#   The sky wa -> s\n",
    "#   he sky was ->  \n",
    "#   e sky was  -> f\n",
    "#    sky was f -> a\n",
    "#   sky was fa -> l\n",
    "SEQLEN = 10\n",
    "STEP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_chars = []\n",
    "label_chars = []\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i:i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to vectorize these input and label texts. Each row of the input to the LSTM corresponds to one of the input texts shown previously. There are `SEQLEN` characters in this input, and since our vocabulary size is given by `nb_chars`, we represent each input character as a one-hot encoded vector of size (`nb_chars`). Thus each input row is a tensor of size (`SEQLEN` and `nb_chars`). Our output label is a single character, so similar to the way we represent each character of our input, it is represented as a one-hot vector of size (`nb_chars`). Thus, the shape of each label is `nb_chars`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X shape to be [samples, time steps, features]\n",
    "# Y shape to be [samples, features]\n",
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to build our model. We define the LSTM's output dimension to have a size of 128. This is a hyper-parameter that needs to be determined by experimentation. In general, if we choose too small a size, then the model does not have sufficient capacity for generating good text, and you will see long runs of repeating characters or runs of repeating word groups. On the other hand, if the value chosen is too large, the model has too many parameters and needs a lot more data to train effectively. We want to return a single character as output, not a sequence of characters, so `return_sequences=False`. We have already seen that the input to the LSTM is of shape (`SEQLEN` and `nb_chars`). In addition, we set `unroll=True` because it improves performance on the TensorFlow backend.\n",
    "\n",
    "The LSTM is connected to a dense (fully connected) layer. The dense layer has (`nb_char`) units, which emits scores for each of the characters in the vocabulary. The activation on the dense layer is a `softmax`, which normalizes the scores to probabilities. The character with the highest probability is chosen as the prediction. We compile the model with the categorical cross-entropy loss function, a good loss function for categorical outputs, and the RMSprop optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model. We use a single RNN with a fully connected layer\n",
    "# to compute the most likely predicted output char\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "NUM_ITERATIONS = 25\n",
    "NUM_EPOCHS_PER_ITERATION = 1\n",
    "NUM_PREDS_PER_EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=HIDDEN_SIZE, return_sequences=True, input_shape=(SEQLEN, nb_chars), unroll=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=HIDDEN_SIZE, return_sequences=False, unroll=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training approach is a little different from what we have seen so far. So far our approach has been to train a model for a fixed number of epochs, then evaluate it against a portion of held-out test data. Since we don't have any labeled data here, we train the model for an epoch (`NUM_EPOCHS_PER_ITERATION=1`) then test it. We continue training like this for 25 (`NUM_ITERATIONS=25`) iterations, stopping once we see intelligible output. So effectively, we are training for `NUM_ITERATIONS` epochs and testing the model after each epoch.\n",
    "\n",
    "Our test consists of generating a character from the model given a random input, then dropping the first character from the input and appending the predicted character from our previous run, and generating another character from the model. We continue this 100 times (`NUM_PREDS_PER_EPOCH=100`) and generate and print the resulting string. The string gives us an indication of the quality of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Iteration #: 0\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 124s 764us/step - loss: 2.2660\n",
      "Generating from seed: muttering \n",
      "muttering to the work the work the work the work the work the work the work the work the work the work the wor==================================================\n",
      "Iteration #: 1\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 118s 728us/step - loss: 1.8275\n",
      "Generating from seed: hat i’m go\n",
      "hat i’m goon the dormouse the dormouse the dormouse the dormouse the dormouse the dormouse the dormouse the do==================================================\n",
      "Iteration #: 2\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 118s 730us/step - loss: 1.6814\n",
      "Generating from seed: ce looked \n",
      "ce looked to the work and the mock turtle the mock turtle the mock turtle the mock turtle the mock turtle the ==================================================\n",
      "Iteration #: 3\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 117s 725us/step - loss: 1.5985\n",
      "Generating from seed:  ‘you are,\n",
      " ‘you are, and the door of the time the door of the time the door of the time the door of the time the door of==================================================\n",
      "Iteration #: 4\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 118s 730us/step - loss: 1.5415\n",
      "Generating from seed: ep instant\n",
      "ep instanted and the mouse to herself, ‘i wonder the mouse to herself, ‘i wonder the mouse to herself, ‘i wond==================================================\n",
      "Iteration #: 5\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 118s 729us/step - loss: 1.5047\n",
      "Generating from seed: gain!’ sai\n",
      "gain!’ said the mock turtle to see the mock turtle to see the mock turtle to see the mock turtle to see the mo==================================================\n",
      "Iteration #: 6\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 118s 727us/step - loss: 1.4708\n",
      "Generating from seed: e this las\n",
      "e this last the sort of the sort of the sort of the sort of the sort of the sort of the sort of the sort of th==================================================\n",
      "Iteration #: 7\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 117s 725us/step - loss: 1.4493\n",
      "Generating from seed: ative work\n",
      "ative works to see the dormouse to see the dormouse to see the dormouse to see the dormouse to see the dormous==================================================\n",
      "Iteration #: 8\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 117s 726us/step - loss: 1.4273\n",
      "Generating from seed: ’ so alice\n",
      "’ so alice to her seemed to the work as she was a little bit the same the same the same the same the same the ==================================================\n",
      "Iteration #: 9\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 118s 727us/step - loss: 1.4093\n",
      "Generating from seed:  you didn’\n",
      " you didn’t have to see the cat out of the soldiers of the court, and the mock turtle so she could not a littl==================================================\n",
      "Iteration #: 10\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 117s 722us/step - loss: 1.3929\n",
      "Generating from seed: ttle. ‘’ti\n",
      "ttle. ‘’tis all the things and she was so the same the mock turtle said to herself to the things and she was s==================================================\n",
      "Iteration #: 11\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 117s 725us/step - loss: 1.3789\n",
      "Generating from seed: compliance\n",
      "compliance with a serpent the white rabbit was a little bit of the words of the words of the words of the word==================================================\n",
      "Iteration #: 12\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 117s 724us/step - loss: 1.3648\n",
      "Generating from seed: ither with\n",
      "ither with the time the mock turtle said to the mock turtle said to the mock turtle said to the mock turtle sa==================================================\n",
      "Iteration #: 13\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 117s 723us/step - loss: 1.3592\n",
      "Generating from seed: at sort it\n",
      "at sort it was a little she was a little she was a little she was a little she was a little she was a little s==================================================\n",
      "Iteration #: 14\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 117s 723us/step - loss: 1.3451\n",
      "Generating from seed: ft off whe\n",
      "ft off when i should be the sort of the works and the copyright arouse to see the white rabbit was a little sh==================================================\n",
      "Iteration #: 15\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 118s 727us/step - loss: 1.3386\n",
      "Generating from seed: t countrie\n",
      "t countried of the work as she was so she was so she was so she was so she was so she was so she was so she wa==================================================\n",
      "Iteration #: 16\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 117s 723us/step - loss: 1.3276\n",
      "Generating from seed: ng, not gr\n",
      "ng, not gringed to the mouse of the same thing i should be the same thing i should be the same thing i should ==================================================\n",
      "Iteration #: 17\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 225s 1ms/step - loss: 1.3198\n",
      "Generating from seed:  said alic\n",
      " said alice, and the mouse was a little thing in the door of the works and the mouse was a little thing in the==================================================\n",
      "Iteration #: 18\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 118s 727us/step - loss: 1.3111\n",
      "Generating from seed: wasn’t ver\n",
      "wasn’t very sure the hatter was the white rabbit was the hatter was the white rabbit was the hatter was the wh==================================================\n",
      "Iteration #: 19\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 118s 728us/step - loss: 1.3085\n",
      "Generating from seed:  being hel\n",
      " being help the mock turtle was a little bill as she was so she was so she was so she was so she was so she wa==================================================\n",
      "Iteration #: 20\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 226s 1ms/step - loss: 1.3013\n",
      "Generating from seed:  ‘if you c\n",
      " ‘if you can’t sure it was a little bit, and the white rabbit with the time the mock turtle said to herself to==================================================\n",
      "Iteration #: 21\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 219s 1ms/step - loss: 1.2948\n",
      "Generating from seed: lice, as s\n",
      "lice, as she said to herself to the treations of the treations of the treations of the treations of the treati==================================================\n",
      "Iteration #: 22\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 117s 724us/step - loss: 1.2918\n",
      "Generating from seed:  about in \n",
      " about in a little bit, and then alice to herself to the little girl of the top of the time the work as she wa==================================================\n",
      "Iteration #: 23\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 117s 720us/step - loss: 1.2845\n",
      "Generating from seed: t puzzled \n",
      "t puzzled at the trees and the mock turtle said to herself, ‘i don’t know what they were the work and the work==================================================\n",
      "Iteration #: 24\n",
      "Epoch 1/1\n",
      "161794/161794 [==============================] - 119s 738us/step - loss: 1.2812\n",
      "Generating from seed: o among ma\n",
      "o among mad a constant to the white rabbit was a little birds and the mock turtle said to herself, ‘i won’t be\n"
     ]
    }
   ],
   "source": [
    "# We train the model in batches and test output generated at each step\n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Iteration #: %d\" % (iteration))\n",
    "    \n",
    "    model.fit(X, y, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "    \n",
    "    # testing model\n",
    "    # randomly choose a row from input_chars, then use it to \n",
    "    # generate text from model for next 100 chars\n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "    print(\"Generating from seed: %s\" % (test_chars))\n",
    "    print(test_chars, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "        X_test = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for i, ch in enumerate(test_chars):\n",
    "            X_test[0, i, char2index[ch]] = 1\n",
    "        pred = model.predict(X_test, verbose=0)[0]\n",
    "        ypred = index2char[np.argmax(pred)]\n",
    "        print(ypred, end=\"\")\n",
    "        # move forward with test_chars + ypred\n",
    "        test_chars = test_chars[1:] + ypred\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the next character or next word of text is not the only thing you can do with this sort of model. This kind of model has been successfully used to make stock predictions (for more information refer to the article: *Financial Market Time Series Prediction with Recurrent Neural Networks*, by A. Bernal, S. Fok, and R. Pidaparthi, 2012) and generate classical music (for more information refer to the article: *DeepBach: A Steerable Model for Bach Chorales Generation*, by G. Hadjeres and F. Pachet, arXiv:1612.01010, 2016), to name a few interesting applications. Andrej Karpathy covers a few other fun examples, such as generating fake Wikipedia pages, algebraic geometry proofs, and Linux source code in his blog post at: *The Unreasonable Effectiveness of Recurrent Neural Networks at http://karpathy.github.io/2015/05/21/rnn-effectiveness/*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p35)",
   "language": "python",
   "name": "conda_tensorflow_p35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
