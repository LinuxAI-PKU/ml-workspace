{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Multi GPU with Keras(Tensorflow)\n",
    "A ConvNet for MNIST digit classification.\n",
    "\n",
    "Multi GPU example with Keras backed by (utilising local tower architecture of TensorFlow for each GPU).\n",
    "\n",
    "Keras performs the best. It utilises the MultiGPU code from: https://github.com/kuza55/keras-extras\n",
    "\n",
    "Specifically, this function implements single-machine multi-GPU data parallelism. It works in the following way:\n",
    "\n",
    "- Divide the model's input(s) into multiple sub-batches.\n",
    "- Apply a model copy on each sub-batch. Every model copy is executed on a dedicated GPU.\n",
    "- Concatenate the results (on CPU) into one big batch.\n",
    "\n",
    "E.g. if your batch_size is 64 and you use gpus=2, then we will divide the input into 2 sub-batches of 32 samples, process each sub-batch on one GPU, then return the full batch of 64 processed samples.\n",
    "\n",
    "This induces quasi-linear speedup on up to 8 GPUs.\n",
    "\n",
    "This function is only available with the TensorFlow backend for the time being."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model Using Multiple GPU Cards\n",
    "\n",
    "Modern workstations may contain multiple GPUs for scientific computation.\n",
    "TensorFlow can leverage this environment to run the training operation\n",
    "concurrently across multiple cards.\n",
    "\n",
    "Training a model in a parallel, distributed fashion requires\n",
    "coordinating training processes. For what follows we term *model replica*\n",
    "to be one copy of a model training on a subset of data.\n",
    "\n",
    "Naively employing asynchronous updates of model parameters\n",
    "leads to sub-optimal training performance\n",
    "because an individual model replica might be trained on a stale\n",
    "copy of the model parameters. Conversely, employing fully synchronous\n",
    "updates will be as slow as the slowest model replica.\n",
    "\n",
    "In a workstation with multiple GPU cards, each GPU will have similar speed\n",
    "and contain enough memory to run an entire MNIST model. Thus, we opt to\n",
    "design our training system in the following manner:\n",
    "\n",
    "* Place an individual model replica on each GPU.\n",
    "* Update model parameters synchronously by waiting for all GPUs to finish\n",
    "processing a batch of data.\n",
    "\n",
    "Here is a diagram of this model:\n",
    "\n",
    "<div style=\"width:40%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n",
    "  <img style=\"width:100%\" src=\"./images/Parallelism.png\">\n",
    "</div>\n",
    "\n",
    "Note that each GPU computes inference as well as the gradients for a unique\n",
    "batch of data. This setup effectively permits dividing up a larger batch\n",
    "of data across the GPUs.\n",
    "\n",
    "This setup requires that all GPUs share the model parameters. A well-known\n",
    "fact is that transferring data to and from GPUs is quite slow. For this\n",
    "reason, we decide to store and update all model parameters on the CPU (see\n",
    "green box). A fresh set of model parameters is transferred to the GPU\n",
    "when a new batch of data is processed by all GPUs.\n",
    "\n",
    "The GPUs are synchronized in operation. All gradients are accumulated from\n",
    "the GPUs and averaged (see green box). The model parameters are updated with\n",
    "the gradients averaged across all model replicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/gpu:0']\n"
     ]
    }
   ],
   "source": [
    "print(get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPUs\n"
     ]
    }
   ],
   "source": [
    "ngpus = len(get_available_gpus()) # int(1)\n",
    "print(\"Using %i GPUs\" %ngpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if K.image_dim_ordering() == 'th':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the base model under a CPU device scope,\n",
    "# so that the model's weights are hosted on CPU memory.\n",
    "# Otherwise they may end up hosted on a GPU, which would\n",
    "# complicate weight sharing.\n",
    "with tf.device('/cpu:0'):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), padding='valid', input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(256, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "For multi-gpu usage to be effective, call `multi_gpu_model` with `gpus >= 2`. Received: `gpus=1`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-2bfee71b7722>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Replicates the model on 8 GPUs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# This assumes that your machine has 8 available GPUs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mparallel_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmulti_gpu_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mngpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mparallel_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rmsprop'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     parallel_model.fit(X_train, Y_train, batch_size=batch_size*ngpus, epochs=nb_epoch,\n",
      "\u001b[1;32mD:\\Work\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\utils\\training_utils.py\u001b[0m in \u001b[0;36mmulti_gpu_model\u001b[1;34m(model, gpus)\u001b[0m\n\u001b[0;32m    107\u001b[0m             raise ValueError('For multi-gpu usage to be effective, '\n\u001b[0;32m    108\u001b[0m                              \u001b[1;34m'call `multi_gpu_model` with `gpus >= 2`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                              'Received: `gpus=%d`' % gpus)\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[0mnum_gpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0mtarget_gpu_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_gpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: For multi-gpu usage to be effective, call `multi_gpu_model` with `gpus >= 2`. Received: `gpus=1`"
     ]
    }
   ],
   "source": [
    "if ngpus > 0:\n",
    "    from keras.utils import multi_gpu_model\n",
    "    # Replicates the model on 8 GPUs.\n",
    "    # This assumes that your machine has 8 available GPUs.\n",
    "    parallel_model = multi_gpu_model(model, gpus=ngpus)\n",
    "    parallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    # This `fit` call will be distributed on 8 GPUs.\n",
    "    # Since the batch size is 256, each GPU will process 32 samples.\n",
    "    parallel_model.fit(X_train, Y_train, batch_size=batch_size*ngpus, epochs=nb_epoch,\n",
    "              verbose=2, validation_data=(X_test, Y_test))#, callbacks=[tensorboard])\n",
    "    score = parallel_model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    duration = time.time() - start_time\n",
    "    print('Total Duration (%.3f sec)' % duration)\n",
    "\n",
    "else:\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "              verbose=2, validation_data=(X_test, Y_test))#, callbacks=[tensorboard])\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    duration = time.time() - start_time\n",
    "    print('Total Duration (%.3f sec)' % duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model via the template model (which shares the same weights):\n",
    "model.save('multi_gpu_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
