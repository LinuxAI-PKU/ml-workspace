{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Multi GPU with Keras (Tensorflow backend)\n",
    "\n",
    "Multi GPU example with Keras (utilising local tower architecture of TensorFlow for each GPU). Keras introduced the `multi_gpu_model` in v 2.0.9 which utilises the MultiGPU code from: https://github.com/kuza55/keras-extras\n",
    "\n",
    "Specifically, the `keras.utils.multi_gpu_model(model, gpus) function implements single-machine multi-GPU data parallelism. It works in the following way:\n",
    "\n",
    "- Divide the model's input(s) into multiple sub-batches.\n",
    "- Apply a model copy on each sub-batch. Every model copy is executed on a dedicated GPU.\n",
    "- Concatenate the results (on CPU) into one big batch.\n",
    "\n",
    "E.g. if our batch_size is 64 and we use gpus=2, then we will divide the input into 2 sub-batches of 32 samples, process each sub-batch on one GPU, then return the full batch of 64 processed samples.\n",
    "\n",
    "This function is only available with the TensorFlow backend for the time being.\n",
    "\n",
    "Here we test a ConvNet for MNIST digit classification. Using multi_gpu_model induces a quasi-linear speedup on up to 8 GPUs.\n",
    "\n",
    "This notebook is compiled from the folowing tutorials\n",
    "https://keras.io/utils/\n",
    "https://www.pyimagesearch.com/2017/10/30/how-to-multi-gpu-training-with-keras-python-and-deep-learning/\n",
    "https://github.com/normanheckscher/mnist-multi-gpu/blob/master/mnist_multi_gpu_keras.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model Using Multiple GPU Cards\n",
    "\n",
    "Modern workstations may contain multiple GPUs for scientific computation.\n",
    "TensorFlow can leverage this environment to run the training operation\n",
    "concurrently across multiple cards.\n",
    "\n",
    "Training a model in a parallel, distributed fashion requires\n",
    "coordinating training processes. For what follows we term *model replica*\n",
    "to be one copy of a model training on a subset of data.\n",
    "\n",
    "Naively employing asynchronous updates of model parameters\n",
    "leads to sub-optimal training performance\n",
    "because an individual model replica might be trained on a stale\n",
    "copy of the model parameters. Conversely, employing fully synchronous\n",
    "updates will be as slow as the slowest model replica.\n",
    "\n",
    "In a workstation with multiple GPU cards, each GPU will have similar speed\n",
    "and contain enough memory to run an entire MNIST model. Thus, we opt to\n",
    "design our training system in the following manner:\n",
    "\n",
    "* Place an individual model replica on each GPU.\n",
    "* Update model parameters synchronously by waiting for all GPUs to finish\n",
    "processing a batch of data.\n",
    "\n",
    "Here is a diagram of this model:\n",
    "\n",
    "<div style=\"width:40%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n",
    "  <img style=\"width:100%\" src=\"./images/Parallelism.png\">\n",
    "</div>\n",
    "\n",
    "Note that each GPU computes inference as well as the gradients for a unique\n",
    "batch of data. This setup effectively permits dividing up a larger batch\n",
    "of data across the GPUs.\n",
    "\n",
    "This setup requires that all GPUs share the model parameters. A well-known\n",
    "fact is that transferring data to and from GPUs is quite slow. For this\n",
    "reason, we decide to store and update all model parameters on the CPU (see\n",
    "green box). A fresh set of model parameters is transferred to the GPU\n",
    "when a new batch of data is processed by all GPUs.\n",
    "\n",
    "The GPUs are synchronized in operation. All gradients are accumulated from\n",
    "the GPUs and averaged (see green box). The model parameters are updated with\n",
    "the gradients averaged across all model replicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check how many GPUs are available in the box\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3', '/device:GPU:4', '/device:GPU:5', '/device:GPU:6', '/device:GPU:7']\n"
     ]
    }
   ],
   "source": [
    "print(get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 GPUs.\n"
     ]
    }
   ],
   "source": [
    "ngpus = len(get_available_gpus()) # int(1)\n",
    "print(\"Using %i GPUs.\" %ngpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if K.image_dim_ordering() == 'th':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), padding='valid', input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(256, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: apparently batch normalization works better in practice after the activation function. https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(batch_size, nb_epoch, ngpus=0):\n",
    "    \n",
    "    if ngpus >= 2:\n",
    "        # Instantiate the base model under a CPU device scope,\n",
    "        # so that the model's weights are hosted on CPU memory.\n",
    "        # Otherwise they may end up hosted on a GPU, which would\n",
    "        # complicate weight sharing.\n",
    "        with tf.device('/cpu:0'):\n",
    "            model = create_model()\n",
    "            \n",
    "        from keras.utils import multi_gpu_model\n",
    "        # Replicates the model on 8 GPUs. This was run on an AWS p2.8xlarge instance.\n",
    "        print('Using Multi-GPU: %i GPUs' %ngpus)\n",
    "        parallel_model = multi_gpu_model(model, gpus=ngpus)\n",
    "        parallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # This `fit` call will be distributed on 8 GPUs.\n",
    "        # if the batch size is 128, each GPU will process 16 samples.\n",
    "        parallel_model.fit(X_train, Y_train, batch_size=batch_size*ngpus, epochs=nb_epoch,\n",
    "                  verbose=2, validation_data=(X_test, Y_test))\n",
    "        score = parallel_model.evaluate(X_test, Y_test, verbose=0)\n",
    "        print('Test score:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "        duration = time.time() - start_time\n",
    "        print('Total Duration (%.3f sec)' % duration)\n",
    "\n",
    "    else:\n",
    "        model = create_model()\n",
    "        \n",
    "        print('NOT Using Multi-GPU')\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "                  verbose=2, validation_data=(X_test, Y_test))\n",
    "        score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "        print('Test score:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "        duration = time.time() - start_time\n",
    "        print('Total Duration (%.3f sec)' % duration)\n",
    "        \n",
    "    # Save model via the base model (which shares the same weights) and not the parallel model:\n",
    "    model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nb_epoch = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT Using Multi-GPU\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      " - 49s - loss: 0.3144 - acc: 0.9281 - val_loss: 1.5856 - val_acc: 0.9184\n",
      "Epoch 2/12\n",
      " - 47s - loss: 0.0782 - acc: 0.9786 - val_loss: 0.0339 - val_acc: 0.9899\n",
      "Epoch 3/12\n",
      " - 47s - loss: 0.0657 - acc: 0.9817 - val_loss: 0.0452 - val_acc: 0.9883\n",
      "Epoch 4/12\n",
      " - 47s - loss: 0.0587 - acc: 0.9846 - val_loss: 0.0425 - val_acc: 0.9891\n",
      "Epoch 5/12\n",
      " - 47s - loss: 0.0546 - acc: 0.9856 - val_loss: 0.0321 - val_acc: 0.9911\n",
      "Epoch 6/12\n",
      " - 47s - loss: 0.0518 - acc: 0.9872 - val_loss: 0.0398 - val_acc: 0.9904\n",
      "Epoch 7/12\n",
      " - 47s - loss: 0.0556 - acc: 0.9860 - val_loss: 0.0395 - val_acc: 0.9919\n",
      "Epoch 8/12\n",
      " - 47s - loss: 0.0570 - acc: 0.9859 - val_loss: 0.0418 - val_acc: 0.9894\n",
      "Epoch 9/12\n",
      " - 47s - loss: 0.0552 - acc: 0.9867 - val_loss: 0.0645 - val_acc: 0.9859\n",
      "Epoch 10/12\n",
      " - 47s - loss: 0.0606 - acc: 0.9864 - val_loss: 0.0519 - val_acc: 0.9893\n",
      "Epoch 11/12\n",
      " - 47s - loss: 0.0577 - acc: 0.9862 - val_loss: 0.0445 - val_acc: 0.9890\n",
      "Epoch 12/12\n",
      " - 47s - loss: 0.0594 - acc: 0.9854 - val_loss: 0.0385 - val_acc: 0.9909\n",
      "Test score: 0.0385208766041\n",
      "Test accuracy: 0.9909\n",
      "Total Duration (574.274 sec)\n"
     ]
    }
   ],
   "source": [
    "# train on one gpu\n",
    "train(batch_size, nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Multi-GPU: 8 GPUs\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      " - 15s - loss: 1.0668 - acc: 0.7690 - val_loss: 0.7550 - val_acc: 0.9724\n",
      "Epoch 2/12\n",
      " - 7s - loss: 0.1344 - acc: 0.9610 - val_loss: 0.0839 - val_acc: 0.9751\n",
      "Epoch 3/12\n",
      " - 7s - loss: 0.0905 - acc: 0.9741 - val_loss: 0.0458 - val_acc: 0.9857\n",
      "Epoch 4/12\n",
      " - 7s - loss: 0.0636 - acc: 0.9818 - val_loss: 0.0415 - val_acc: 0.9881\n",
      "Epoch 5/12\n",
      " - 7s - loss: 0.0495 - acc: 0.9849 - val_loss: 0.0317 - val_acc: 0.9907\n",
      "Epoch 6/12\n",
      " - 7s - loss: 0.0415 - acc: 0.9884 - val_loss: 0.0325 - val_acc: 0.9910\n",
      "Epoch 7/12\n",
      " - 7s - loss: 0.0341 - acc: 0.9902 - val_loss: 0.0574 - val_acc: 0.9854\n",
      "Epoch 8/12\n",
      " - 7s - loss: 0.0303 - acc: 0.9910 - val_loss: 0.0342 - val_acc: 0.9911\n",
      "Epoch 9/12\n",
      " - 7s - loss: 0.0241 - acc: 0.9923 - val_loss: 0.0293 - val_acc: 0.9926\n",
      "Epoch 10/12\n",
      " - 7s - loss: 0.0239 - acc: 0.9930 - val_loss: 0.0252 - val_acc: 0.9925\n",
      "Epoch 11/12\n",
      " - 7s - loss: 0.0211 - acc: 0.9935 - val_loss: 0.0300 - val_acc: 0.9917\n",
      "Epoch 12/12\n",
      " - 7s - loss: 0.0186 - acc: 0.9942 - val_loss: 0.0403 - val_acc: 0.9903\n",
      "Test score: 0.0403047469056\n",
      "Test accuracy: 0.9903\n",
      "Total Duration (99.043 sec)\n"
     ]
    }
   ],
   "source": [
    "# train on all available gpus\n",
    "train(batch_size, nb_epoch, ngpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the quasi-linear speed up in training: Using 8 GPUs, we are able to decrese each epoch to only 7s as compared to 47s with 1 GPU. With 8 GPUs the entire work finished in ~1.5 minutes whereas it took ~9.5 mins with 1 GPU.\n",
    "\n",
    "**Note:** *In this case, the single GPU experiment obtained slightly higher accuracy than the multi-GPU experiment. When training any stochastic machine learning model, there will be some variance. If you were to average these results out across hundreds of runs they would be (approximately) the same.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
